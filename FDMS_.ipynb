{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFCWdfuNba36",
        "outputId": "a4f7ca44-5d27-49be-9d22-e29a23d4a68b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (2.0.40)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.37.26-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy) (4.13.0)\n",
            "Collecting botocore<1.38.0,>=1.37.26 (from boto3)\n",
            "  Downloading botocore-1.37.26-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.26->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.26-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.26-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary, jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.37.26 botocore-1.37.26 jmespath-1.0.1 psycopg2-binary-2.9.10 s3transfer-0.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary pandas sqlalchemy boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_ACCESS_KEY = 'AKIAQH72IQQP77ZGNJFB'\n",
        "AWS_SECRET_KEY = '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'\n",
        "AWS_REGION = 'ap-south-1'\n",
        "BUCKET_NAME = 'testempdoc'\n",
        "\n",
        "print(\"Libraries imported and AWS configured successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT7nDjkZbiLL",
        "outputId": "c505d211-4b1f-4f3e-b1b2-68c65d0d24ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and AWS configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_files():\n",
        "    try:\n",
        "        print(\"Creating test files...\")\n",
        "\n",
        "        # Create test directory\n",
        "        test_dir = Path(\"/Users/espinshalo/Downloads/FDMS/test_documents\")\n",
        "        test_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Define test files and their content\n",
        "        test_files = {\n",
        "            \"Book.xlsx\": \"Sample Excel content for employee records\\nEmployee ID: 001\\nDepartment: HR\",\n",
        "            \"details.pdf\": \"Sample PDF content for employee details\\nEmployee: John Doe\\nPosition: Manager\",\n",
        "            \"Dummy.docx\": \"Sample Word document content\\nProject: FDMS\\nStatus: Active\",\n",
        "            \"payslip.pdf\": \"Sample payslip content\\nEmployee: Jane Smith\\nMonth: April 2024\",\n",
        "            \"position.pdf\": \"Sample position document content\\nRole: Senior Developer\\nDepartment: Engineering\",\n",
        "            \"salary.pdf\": \"Sample salary document content\\nEmployee: Bob Johnson\\nYear: 2024\",\n",
        "            \"first_source_jpg.png\": \"Sample image content for first source\\nType: PNG\\nSize: 9.3KB\",\n",
        "            \"firstsource_logo_jpeg.jpeg\": \"Sample image content for Firstsource Logo\\nType: JPEG\\nSize: 17KB\",\n",
        "            \"firstsource_logo.jpg\": \"Sample image content for Firstsource Logo\\nType: JPG\\nSize: 17KB\"\n",
        "        }\n",
        "\n",
        "        created_files = []\n",
        "        # Create each test file\n",
        "        for filename, content in test_files.items():\n",
        "            file_path = test_dir / filename\n",
        "            with open(file_path, 'w') as f:\n",
        "                f.write(f\"{content}\\nCreated at: {datetime.now()}\")\n",
        "            created_files.append(str(file_path))\n",
        "            print(f\"Created: {file_path}\")\n",
        "\n",
        "        print(f\"\\nCreated {len(created_files)} files successfully!\")\n",
        "        return created_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating test files: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Create test files\n",
        "test_files = create_test_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xx4S7njbrUi",
        "outputId": "a4fbbecb-63bb-4d28-ee9d-b4c07d1239c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating test files...\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/Book.xlsx\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/details.pdf\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/Dummy.docx\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/payslip.pdf\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/position.pdf\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/salary.pdf\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/first_source_jpg.png\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo_jpeg.jpeg\n",
            "Created: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo.jpg\n",
            "\n",
            "Created 9 files successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_old_database():\n",
        "    try:\n",
        "        print(\"Creating old database with test file paths...\")\n",
        "\n",
        "        # Create SQLite database for old system\n",
        "        old_db = create_engine('sqlite:///old_db.db')\n",
        "\n",
        "        # Create tables\n",
        "        with old_db.connect() as conn:\n",
        "            # Drop existing tables\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS employee_documents\"))\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS employees\"))\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS departments\"))\n",
        "\n",
        "            # Create departments table\n",
        "            conn.execute(text(\"\"\"\n",
        "            CREATE TABLE departments (\n",
        "                department_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                department_name TEXT NOT NULL\n",
        "            )\n",
        "            \"\"\"))\n",
        "\n",
        "            # Create employees table\n",
        "            conn.execute(text(\"\"\"\n",
        "            CREATE TABLE employees (\n",
        "                employee_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                first_name TEXT NOT NULL,\n",
        "                last_name TEXT NOT NULL,\n",
        "                email TEXT NOT NULL UNIQUE,\n",
        "                department_id INTEGER,\n",
        "                status TEXT\n",
        "            )\n",
        "            \"\"\"))\n",
        "\n",
        "            # Create employee_documents table\n",
        "            conn.execute(text(\"\"\"\n",
        "            CREATE TABLE employee_documents (\n",
        "                document_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                employee_id INTEGER,\n",
        "                document_type TEXT NOT NULL,\n",
        "                file_path TEXT NOT NULL,\n",
        "                upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "                document_number TEXT,\n",
        "                FOREIGN KEY (employee_id) REFERENCES employees(employee_id)\n",
        "            )\n",
        "            \"\"\"))\n",
        "\n",
        "            # Insert departments\n",
        "            conn.execute(text(\"\"\"\n",
        "            INSERT INTO departments (department_name) VALUES\n",
        "                ('HR'),\n",
        "                ('Finance'),\n",
        "                ('Operations')\n",
        "            \"\"\"))\n",
        "\n",
        "            # Insert employees\n",
        "            conn.execute(text(\"\"\"\n",
        "            INSERT INTO employees\n",
        "            (first_name, last_name, email, department_id, status) VALUES\n",
        "                ('John', 'Doe', 'john.doe@company.com', 1, 'ACTIVE'),\n",
        "                ('Jane', 'Smith', 'jane.smith@company.com', 2, 'ACTIVE'),\n",
        "                ('Bob', 'Johnson', 'bob.johnson@company.com', 3, 'ACTIVE')\n",
        "            \"\"\"))\n",
        "\n",
        "            # Insert documents with test file paths\n",
        "            test_dir = Path(\"/Users/espinshalo/Downloads/FDMS/test_documents\")\n",
        "            documents = [\n",
        "                (1, 'EXCEL', str(test_dir / \"Book.xlsx\"), 'DOC001'),\n",
        "                (1, 'PDF', str(test_dir / \"details.pdf\"), 'DOC002'),\n",
        "                (1, 'IMAGE', str(test_dir / \"first_source_jpg.png\"), 'IMG001'),\n",
        "                (2, 'WORD', str(test_dir / \"Dummy.docx\"), 'DOC003'),\n",
        "                (2, 'PAYSLIP', str(test_dir / \"payslip.pdf\"), 'DOC004'),\n",
        "                (2, 'IMAGE', str(test_dir / \"firstsource_logo_jpeg.jpeg\"), 'IMG002'),\n",
        "                (3, 'POSITION', str(test_dir / \"position.pdf\"), 'DOC005'),\n",
        "                (3, 'SALARY', str(test_dir / \"salary.pdf\"), 'DOC006'),\n",
        "                (3, 'IMAGE', str(test_dir / \"firstsource_logo.jpg\"), 'IMG003')\n",
        "            ]\n",
        "\n",
        "            for emp_id, doc_type, file_path, doc_num in documents:\n",
        "                conn.execute(text(\"\"\"\n",
        "                INSERT INTO employee_documents\n",
        "                (employee_id, document_type, file_path, document_number)\n",
        "                VALUES (:emp_id, :doc_type, :file_path, :doc_num)\n",
        "                \"\"\"), {\n",
        "                    'emp_id': emp_id,\n",
        "                    'doc_type': doc_type,\n",
        "                    'file_path': file_path,\n",
        "                    'doc_num': doc_num\n",
        "                })\n",
        "\n",
        "            conn.commit()\n",
        "\n",
        "        print(\"Old database created successfully!\")\n",
        "\n",
        "        # Verify the data\n",
        "        with old_db.connect() as conn:\n",
        "            print(\"\\nVerifying data in old database:\")\n",
        "            documents = pd.read_sql(\"\"\"\n",
        "                SELECT\n",
        "                    ed.document_id,\n",
        "                    ed.document_type,\n",
        "                    ed.file_path,\n",
        "                    e.first_name,\n",
        "                    e.last_name,\n",
        "                    d.department_name\n",
        "                FROM employee_documents ed\n",
        "                JOIN employees e ON ed.employee_id = e.employee_id\n",
        "                JOIN departments d ON e.department_id = d.department_id\n",
        "            \"\"\", conn)\n",
        "\n",
        "            print(\"\\nDocument Mappings:\")\n",
        "            print(documents)\n",
        "\n",
        "            # Verify file existence\n",
        "            print(\"\\nVerifying file existence:\")\n",
        "            for _, row in documents.iterrows():\n",
        "                file_exists = os.path.exists(row['file_path'])\n",
        "                print(f\"File: {row['file_path']} - {'Exists' if file_exists else 'Not Found'}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating old database: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Create old database\n",
        "create_old_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krZNfWSVbwn8",
        "outputId": "d2f1704b-b1d6-4a05-a752-9ebef42051a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating old database with test file paths...\n",
            "Old database created successfully!\n",
            "\n",
            "Verifying data in old database:\n",
            "\n",
            "Document Mappings:\n",
            "   document_id document_type  \\\n",
            "0            1         EXCEL   \n",
            "1            2           PDF   \n",
            "2            3         IMAGE   \n",
            "3            4          WORD   \n",
            "4            5       PAYSLIP   \n",
            "5            6         IMAGE   \n",
            "6            7      POSITION   \n",
            "7            8        SALARY   \n",
            "8            9         IMAGE   \n",
            "\n",
            "                                           file_path first_name last_name  \\\n",
            "0  /Users/espinshalo/Downloads/FDMS/test_document...       John       Doe   \n",
            "1  /Users/espinshalo/Downloads/FDMS/test_document...       John       Doe   \n",
            "2  /Users/espinshalo/Downloads/FDMS/test_document...       John       Doe   \n",
            "3  /Users/espinshalo/Downloads/FDMS/test_document...       Jane     Smith   \n",
            "4  /Users/espinshalo/Downloads/FDMS/test_document...       Jane     Smith   \n",
            "5  /Users/espinshalo/Downloads/FDMS/test_document...       Jane     Smith   \n",
            "6  /Users/espinshalo/Downloads/FDMS/test_document...        Bob   Johnson   \n",
            "7  /Users/espinshalo/Downloads/FDMS/test_document...        Bob   Johnson   \n",
            "8  /Users/espinshalo/Downloads/FDMS/test_document...        Bob   Johnson   \n",
            "\n",
            "  department_name  \n",
            "0              HR  \n",
            "1              HR  \n",
            "2              HR  \n",
            "3         Finance  \n",
            "4         Finance  \n",
            "5         Finance  \n",
            "6      Operations  \n",
            "7      Operations  \n",
            "8      Operations  \n",
            "\n",
            "Verifying file existence:\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/Book.xlsx - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/details.pdf - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/first_source_jpg.png - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/Dummy.docx - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/payslip.pdf - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo_jpeg.jpeg - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/position.pdf - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/salary.pdf - Exists\n",
            "File: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo.jpg - Exists\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_database():\n",
        "    try:\n",
        "        print(\"Creating new database schema...\")\n",
        "\n",
        "        # Create SQLite database for new system\n",
        "        new_db = create_engine('sqlite:///new_db.db')\n",
        "\n",
        "        # Create document_metadata table\n",
        "        with new_db.connect() as conn:\n",
        "            # Drop existing table if it exists\n",
        "            conn.execute(text(\"DROP TABLE IF EXISTS document_metadata\"))\n",
        "\n",
        "            # Create new table\n",
        "            conn.execute(text(\"\"\"\n",
        "            CREATE TABLE document_metadata (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                document_id TEXT NOT NULL UNIQUE,\n",
        "                employee_id INTEGER NOT NULL,\n",
        "                first_name TEXT NOT NULL,\n",
        "                last_name TEXT NOT NULL,\n",
        "                email TEXT NOT NULL,\n",
        "                department_name TEXT NOT NULL,\n",
        "                document_type TEXT NOT NULL,\n",
        "                document_category TEXT NOT NULL,\n",
        "                document_number TEXT,\n",
        "                original_file_path TEXT NOT NULL,\n",
        "                s3_file_path TEXT,\n",
        "                upload_date TIMESTAMP NOT NULL,\n",
        "                processed_date TIMESTAMP,\n",
        "                status TEXT NOT NULL,\n",
        "                is_active BOOLEAN DEFAULT 1\n",
        "            )\n",
        "            \"\"\"))\n",
        "\n",
        "            conn.commit()\n",
        "\n",
        "        print(\"New database schema created successfully!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating new database: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Create new database schema\n",
        "create_new_database()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbYxnfPeb4VS",
        "outputId": "d7d48ce8-25fd-4e7a-dbf6-a81ce2c94bd2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new database schema...\n",
            "New database schema created successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def migrate_data_to_new_db():\n",
        "    try:\n",
        "        print(\"Starting data migration from old to new database...\")\n",
        "\n",
        "        # Connect to databases\n",
        "        old_db = create_engine('sqlite:///old_db.db')\n",
        "        new_db = create_engine('sqlite:///new_db.db')\n",
        "\n",
        "        # Extract data from old database\n",
        "        query = \"\"\"\n",
        "        SELECT\n",
        "            ed.document_id as old_document_id,\n",
        "            ed.employee_id,\n",
        "            ed.document_type,\n",
        "            ed.file_path,\n",
        "            ed.document_number,\n",
        "            ed.upload_date,\n",
        "            e.first_name,\n",
        "            e.last_name,\n",
        "            e.email,\n",
        "            e.status,\n",
        "            d.department_name\n",
        "        FROM employee_documents ed\n",
        "        JOIN employees e ON ed.employee_id = e.employee_id\n",
        "        JOIN departments d ON e.department_id = d.department_id\n",
        "        \"\"\"\n",
        "\n",
        "        df = pd.read_sql(query, old_db)\n",
        "        print(f\"Found {len(df)} records to migrate\")\n",
        "\n",
        "        # Create a single connection for all operations\n",
        "        with new_db.connect() as conn:\n",
        "            # Start transaction\n",
        "            trans = conn.begin()\n",
        "            try:\n",
        "                for idx, row in df.iterrows():\n",
        "                    # Create unique document ID\n",
        "                    document_id = f\"DOC_{row['employee_id']}_{row['document_type']}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
        "\n",
        "                    # Determine document category\n",
        "                    document_category = 'IDENTIFICATION' if row['document_type'] in ['PASSPORT', 'VISA'] else \\\n",
        "                                     'EMPLOYMENT' if row['document_type'] in ['CONTRACT', 'PAYSLIP', 'SALARY', 'POSITION'] else \\\n",
        "                                     'IMAGE' if row['document_type'] == 'IMAGE' else \\\n",
        "                                     'OTHER'\n",
        "\n",
        "                    # Insert into new database\n",
        "                    conn.execute(text(\"\"\"\n",
        "                    INSERT INTO document_metadata (\n",
        "                        document_id, employee_id, first_name, last_name,\n",
        "                        email, department_name, document_type, document_category,\n",
        "                        document_number, original_file_path, upload_date,\n",
        "                        processed_date, status, is_active\n",
        "                    ) VALUES (\n",
        "                        :doc_id, :emp_id, :fname, :lname,\n",
        "                        :email, :dept, :doc_type, :doc_cat,\n",
        "                        :doc_num, :orig_path, :upload_date,\n",
        "                        :proc_date, :status, :active\n",
        "                    )\n",
        "                    \"\"\"), {\n",
        "                        'doc_id': document_id,\n",
        "                        'emp_id': row['employee_id'],\n",
        "                        'fname': row['first_name'],\n",
        "                        'lname': row['last_name'],\n",
        "                        'email': row['email'],\n",
        "                        'dept': row['department_name'],\n",
        "                        'doc_type': row['document_type'],\n",
        "                        'doc_cat': document_category,\n",
        "                        'doc_num': row['document_number'],\n",
        "                        'orig_path': row['file_path'],\n",
        "                        'upload_date': row['upload_date'],\n",
        "                        'proc_date': datetime.now(),\n",
        "                        'status': 'PENDING_UPLOAD',\n",
        "                        'active': 1\n",
        "                    })\n",
        "\n",
        "                    print(f\"Migrated document: {document_id}\")\n",
        "\n",
        "                # Commit the transaction\n",
        "                trans.commit()\n",
        "                print(\"\\nAll records committed successfully!\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # Rollback in case of error\n",
        "                trans.rollback()\n",
        "                print(f\"Error during migration, rolling back: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "        # Verify migration with a new connection\n",
        "        print(\"\\nVerifying migration...\")\n",
        "        verify_query = \"\"\"\n",
        "        SELECT\n",
        "            document_id,\n",
        "            first_name,\n",
        "            last_name,\n",
        "            document_type,\n",
        "            status,\n",
        "            department_name,\n",
        "            document_category\n",
        "        FROM document_metadata\n",
        "        \"\"\"\n",
        "\n",
        "        with new_db.connect() as conn:\n",
        "            result = pd.read_sql(verify_query, conn)\n",
        "            print(\"\\nMigration Summary:\")\n",
        "            print(f\"Total records migrated: {len(result)}\")\n",
        "\n",
        "            if not result.empty:\n",
        "                print(\"\\nSample of migrated data:\")\n",
        "                print(result.head())\n",
        "\n",
        "                print(\"\\nDocument categories distribution:\")\n",
        "                print(result['document_category'].value_counts())\n",
        "\n",
        "                print(\"\\nDocument types distribution:\")\n",
        "                print(result['document_type'].value_counts())\n",
        "\n",
        "                print(\"\\nStatus distribution:\")\n",
        "                print(result['status'].value_counts())\n",
        "            else:\n",
        "                print(\"No records found in the new database!\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data migration: {str(e)}\")\n",
        "        print(\"Full error details:\", e)\n",
        "        return False\n",
        "\n",
        "# Migrate data to new database\n",
        "migrate_data_to_new_db()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxR9iYQ6cE5E",
        "outputId": "7fc0aa2d-e15f-419d-e611-d3706455198a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data migration from old to new database...\n",
            "Found 9 records to migrate\n",
            "Migrated document: DOC_1_EXCEL_20250403083552\n",
            "Migrated document: DOC_1_PDF_20250403083552\n",
            "Migrated document: DOC_1_IMAGE_20250403083552\n",
            "Migrated document: DOC_2_WORD_20250403083552\n",
            "Migrated document: DOC_2_PAYSLIP_20250403083552\n",
            "Migrated document: DOC_2_IMAGE_20250403083552\n",
            "Migrated document: DOC_3_POSITION_20250403083552\n",
            "Migrated document: DOC_3_SALARY_20250403083552\n",
            "Migrated document: DOC_3_IMAGE_20250403083552\n",
            "\n",
            "All records committed successfully!\n",
            "\n",
            "Verifying migration...\n",
            "\n",
            "Migration Summary:\n",
            "Total records migrated: 9\n",
            "\n",
            "Sample of migrated data:\n",
            "                    document_id first_name last_name document_type  \\\n",
            "0    DOC_1_EXCEL_20250403083552       John       Doe         EXCEL   \n",
            "1      DOC_1_PDF_20250403083552       John       Doe           PDF   \n",
            "2    DOC_1_IMAGE_20250403083552       John       Doe         IMAGE   \n",
            "3     DOC_2_WORD_20250403083552       Jane     Smith          WORD   \n",
            "4  DOC_2_PAYSLIP_20250403083552       Jane     Smith       PAYSLIP   \n",
            "\n",
            "           status department_name document_category  \n",
            "0  PENDING_UPLOAD              HR             OTHER  \n",
            "1  PENDING_UPLOAD              HR             OTHER  \n",
            "2  PENDING_UPLOAD              HR             IMAGE  \n",
            "3  PENDING_UPLOAD         Finance             OTHER  \n",
            "4  PENDING_UPLOAD         Finance        EMPLOYMENT  \n",
            "\n",
            "Document categories distribution:\n",
            "document_category\n",
            "OTHER         3\n",
            "IMAGE         3\n",
            "EMPLOYMENT    3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Document types distribution:\n",
            "document_type\n",
            "IMAGE       3\n",
            "EXCEL       1\n",
            "PDF         1\n",
            "WORD        1\n",
            "PAYSLIP     1\n",
            "POSITION    1\n",
            "SALARY      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Status distribution:\n",
            "status\n",
            "PENDING_UPLOAD    9\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_s3():\n",
        "    try:\n",
        "        print(\"Starting S3 upload process...\")\n",
        "\n",
        "        # Initialize S3 client\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY,\n",
        "            aws_secret_access_key=AWS_SECRET_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "\n",
        "        # Test S3 connection\n",
        "        try:\n",
        "            s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
        "            print(\"Successfully connected to S3 bucket!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to S3: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "        # Connect to new database\n",
        "        new_db = create_engine('sqlite:///new_db.db')\n",
        "\n",
        "        # Get pending uploads\n",
        "        with new_db.connect() as conn:\n",
        "            df = pd.read_sql(\"SELECT * FROM document_metadata WHERE status = 'PENDING_UPLOAD'\", conn)\n",
        "\n",
        "        print(f\"Found {len(df)} documents to upload\")\n",
        "\n",
        "        success_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                # Check if file exists\n",
        "                if not os.path.exists(row['original_file_path']):\n",
        "                    print(f\"File not found: {row['original_file_path']}\")\n",
        "                    error_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Get file extension\n",
        "                _, file_extension = os.path.splitext(row['original_file_path'])\n",
        "\n",
        "                # Create S3 key\n",
        "                s3_key = f\"{row['department_name'].lower()}/{row['document_type'].lower()}/{row['document_id']}{file_extension}\"\n",
        "\n",
        "                # Upload to S3\n",
        "                print(f\"Uploading: {row['original_file_path']} to s3://{BUCKET_NAME}/{s3_key}\")\n",
        "                s3_client.upload_file(\n",
        "                    row['original_file_path'],\n",
        "                    BUCKET_NAME,\n",
        "                    s3_key,\n",
        "                    ExtraArgs={\n",
        "                        'Metadata': {\n",
        "                            'employee_id': str(row['employee_id']),\n",
        "                            'document_type': row['document_type'],\n",
        "                            'department': row['department_name'],\n",
        "                            'document_number': row['document_number']\n",
        "                        }\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Update database\n",
        "                with new_db.connect() as conn:\n",
        "                    conn.execute(text(\"\"\"\n",
        "                    UPDATE document_metadata\n",
        "                    SET s3_file_path = :s3_path,\n",
        "                        status = 'UPLOADED',\n",
        "                        processed_date = :proc_date\n",
        "                    WHERE document_id = :doc_id\n",
        "                    \"\"\"), {\n",
        "                        's3_path': f\"s3://{BUCKET_NAME}/{s3_key}\",\n",
        "                        'proc_date': datetime.now(),\n",
        "                        'doc_id': row['document_id']\n",
        "                    })\n",
        "\n",
        "                success_count += 1\n",
        "                print(f\"Successfully uploaded: {row['document_id']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                print(f\"Error uploading document {row['document_id']}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(\"\\nUpload Summary:\")\n",
        "        print(f\"Total documents processed: {len(df)}\")\n",
        "        print(f\"Successfully uploaded: {success_count}\")\n",
        "        print(f\"Failed: {error_count}\")\n",
        "\n",
        "        # Final status check\n",
        "        with new_db.connect() as conn:\n",
        "            status_df = pd.read_sql(\"\"\"\n",
        "                SELECT status, COUNT(*) as count\n",
        "                FROM document_metadata\n",
        "                GROUP BY status\n",
        "            \"\"\", conn)\n",
        "            print(\"\\nFinal Status Distribution:\")\n",
        "            print(status_df)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during S3 upload: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Upload documents to S3\n",
        "upload_to_s3()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJhYYfuHcKdu",
        "outputId": "56a5df2c-331c-455e-8ead-9eef0a13a008"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting S3 upload process...\n",
            "Successfully connected to S3 bucket!\n",
            "Found 9 documents to upload\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/Book.xlsx to s3://testempdoc/hr/excel/DOC_1_EXCEL_20250403083552.xlsx\n",
            "Successfully uploaded: DOC_1_EXCEL_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/details.pdf to s3://testempdoc/hr/pdf/DOC_1_PDF_20250403083552.pdf\n",
            "Successfully uploaded: DOC_1_PDF_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/first_source_jpg.png to s3://testempdoc/hr/image/DOC_1_IMAGE_20250403083552.png\n",
            "Successfully uploaded: DOC_1_IMAGE_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/Dummy.docx to s3://testempdoc/finance/word/DOC_2_WORD_20250403083552.docx\n",
            "Successfully uploaded: DOC_2_WORD_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/payslip.pdf to s3://testempdoc/finance/payslip/DOC_2_PAYSLIP_20250403083552.pdf\n",
            "Successfully uploaded: DOC_2_PAYSLIP_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo_jpeg.jpeg to s3://testempdoc/finance/image/DOC_2_IMAGE_20250403083552.jpeg\n",
            "Successfully uploaded: DOC_2_IMAGE_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/position.pdf to s3://testempdoc/operations/position/DOC_3_POSITION_20250403083552.pdf\n",
            "Successfully uploaded: DOC_3_POSITION_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/salary.pdf to s3://testempdoc/operations/salary/DOC_3_SALARY_20250403083552.pdf\n",
            "Successfully uploaded: DOC_3_SALARY_20250403083552\n",
            "Uploading: /Users/espinshalo/Downloads/FDMS/test_documents/firstsource_logo.jpg to s3://testempdoc/operations/image/DOC_3_IMAGE_20250403083552.jpg\n",
            "Successfully uploaded: DOC_3_IMAGE_20250403083552\n",
            "\n",
            "Upload Summary:\n",
            "Total documents processed: 9\n",
            "Successfully uploaded: 9\n",
            "Failed: 0\n",
            "\n",
            "Final Status Distribution:\n",
            "           status  count\n",
            "0  PENDING_UPLOAD      9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_complete_process():\n",
        "    try:\n",
        "        print(\"Verifying complete migration process...\")\n",
        "\n",
        "        # Connect to databases\n",
        "        old_db = create_engine('sqlite:///old_db.db')\n",
        "        new_db = create_engine('sqlite:///new_db.db')\n",
        "\n",
        "        # Check old database\n",
        "        with old_db.connect() as conn:\n",
        "            old_count = pd.read_sql(\"SELECT COUNT(*) as count FROM employee_documents\", conn).iloc[0]['count']\n",
        "            print(f\"\\nOld database document count: {old_count}\")\n",
        "\n",
        "            print(\"\\nOld database document types:\")\n",
        "            old_types = pd.read_sql(\"\"\"\n",
        "                SELECT document_type, COUNT(*) as count\n",
        "                FROM employee_documents\n",
        "                GROUP BY document_type\n",
        "            \"\"\", conn)\n",
        "            print(old_types)\n",
        "\n",
        "        # Check new database\n",
        "        with new_db.connect() as conn:\n",
        "            new_df = pd.read_sql(\"\"\"\n",
        "                SELECT\n",
        "                    document_type,\n",
        "                    document_category,\n",
        "                    status,\n",
        "                    COUNT(*) as count\n",
        "                FROM document_metadata\n",
        "                GROUP BY document_type, document_category, status\n",
        "            \"\"\", conn)\n",
        "            print(\"\\nNew database status summary:\")\n",
        "            print(new_df)\n",
        "\n",
        "        # Check S3\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY,\n",
        "            aws_secret_access_key=AWS_SECRET_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = s3_client.list_objects_v2(Bucket=BUCKET_NAME)\n",
        "            s3_count = response.get('KeyCount', 0)\n",
        "            print(f\"\\nS3 document count: {s3_count}\")\n",
        "\n",
        "            if 'Contents' in response:\n",
        "                print(\"\\nS3 files by department:\")\n",
        "                s3_files = {}\n",
        "                for obj in response['Contents']:\n",
        "                    dept = obj['Key'].split('/')[0]\n",
        "                    s3_files[dept] = s3_files.get(dept, 0) + 1\n",
        "\n",
        "                for dept, count in s3_files.items():\n",
        "                    print(f\"{dept}: {count} files\")\n",
        "\n",
        "                print(\"\\nSample S3 files:\")\n",
        "                for obj in response['Contents'][:5]:\n",
        "                    print(f\"- {obj['Key']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error checking S3: {str(e)}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during verification: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Verify complete process\n",
        "verify_complete_process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIspUn8ScPPX",
        "outputId": "6810114c-342a-4cc1-fcb7-8808a03d6c3c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying complete migration process...\n",
            "\n",
            "Old database document count: 9\n",
            "\n",
            "Old database document types:\n",
            "  document_type  count\n",
            "0         EXCEL      1\n",
            "1         IMAGE      3\n",
            "2       PAYSLIP      1\n",
            "3           PDF      1\n",
            "4      POSITION      1\n",
            "5        SALARY      1\n",
            "6          WORD      1\n",
            "\n",
            "New database status summary:\n",
            "  document_type document_category          status  count\n",
            "0         EXCEL             OTHER  PENDING_UPLOAD      1\n",
            "1         IMAGE             IMAGE  PENDING_UPLOAD      3\n",
            "2       PAYSLIP        EMPLOYMENT  PENDING_UPLOAD      1\n",
            "3           PDF             OTHER  PENDING_UPLOAD      1\n",
            "4      POSITION        EMPLOYMENT  PENDING_UPLOAD      1\n",
            "5        SALARY        EMPLOYMENT  PENDING_UPLOAD      1\n",
            "6          WORD             OTHER  PENDING_UPLOAD      1\n",
            "\n",
            "S3 document count: 29\n",
            "\n",
            "S3 files by department:\n",
            "engineering: 3 files\n",
            "finance: 9 files\n",
            "hr: 9 files\n",
            "marketing: 1 files\n",
            "operations: 7 files\n",
            "\n",
            "Sample S3 files:\n",
            "- engineering/payslip/DOC_2_PAYSLIP_20250401112013.txt\n",
            "- engineering/resignation/DOC_4_RESIGNATION_20250401112013.txt\n",
            "- engineering/visa/DOC_2_VISA_20250401112013.txt\n",
            "- finance/contract/DOC_3_CONTRACT_20250401112013.txt\n",
            "- finance/image/DOC_2_IMAGE_20250403083552.jpeg\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_s3_files():\n",
        "    \"\"\"\n",
        "    Lists all files in the S3 bucket with complete S3 URLs for downloading.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create S3 client with your credentials\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=AWS_ACCESS_KEY,\n",
        "            aws_secret_access_key=AWS_SECRET_KEY,\n",
        "            region_name=AWS_REGION\n",
        "        )\n",
        "\n",
        "        # S3 bucket details\n",
        "        bucket_name = BUCKET_NAME\n",
        "        region = AWS_REGION\n",
        "\n",
        "        print(\"Listing all files in S3 bucket with download URLs...\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # List all objects in the bucket\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        pages = paginator.paginate(Bucket=bucket_name)\n",
        "\n",
        "        # Dictionary to store files by department\n",
        "        files_by_dept = {}\n",
        "\n",
        "        for page in pages:\n",
        "            if 'Contents' in page:\n",
        "                for obj in page['Contents']:\n",
        "                    # Get the key (file path)\n",
        "                    key = obj['Key']\n",
        "\n",
        "                    # Skip if it's a directory marker\n",
        "                    if key.endswith('/'):\n",
        "                        continue\n",
        "\n",
        "                    # Extract department and document type from the path\n",
        "                    parts = key.split('/')\n",
        "                    if len(parts) >= 2:\n",
        "                        dept = parts[0]\n",
        "                        doc_type = parts[1] if len(parts) > 1 else 'Unknown'\n",
        "\n",
        "                        # Generate the complete S3 URL\n",
        "                        s3_url = f\"https://{bucket_name}.s3.{region}.amazonaws.com/{key}\"\n",
        "\n",
        "                        if dept not in files_by_dept:\n",
        "                            files_by_dept[dept] = {}\n",
        "                        if doc_type not in files_by_dept[dept]:\n",
        "                            files_by_dept[dept][doc_type] = []\n",
        "\n",
        "                        files_by_dept[dept][doc_type].append({\n",
        "                            'file_name': parts[-1],\n",
        "                            'size': obj['Size'],\n",
        "                            'last_modified': obj['LastModified'],\n",
        "                            's3_path': key,\n",
        "                            'download_url': s3_url\n",
        "                        })\n",
        "\n",
        "        # Print the organized list\n",
        "        for dept in sorted(files_by_dept.keys()):\n",
        "            print(f\"\\nDepartment: {dept.upper()}\")\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "            for doc_type in sorted(files_by_dept[dept].keys()):\n",
        "                print(f\"\\nDocument Type: {doc_type}\")\n",
        "                print(\"-\" * 30)\n",
        "\n",
        "                for file_info in files_by_dept[dept][doc_type]:\n",
        "                    print(f\"File: {file_info['file_name']}\")\n",
        "                    print(f\"Size: {file_info['size']} bytes\")\n",
        "                    print(f\"Last Modified: {file_info['last_modified']}\")\n",
        "                    print(f\"S3 Path: {file_info['s3_path']}\")\n",
        "                    print(f\"Download URL: {file_info['download_url']}\")\n",
        "                    print()\n",
        "\n",
        "        # Print summary\n",
        "        total_files = sum(len(files)\n",
        "                         for dept in files_by_dept.values()\n",
        "                         for files in dept.values())\n",
        "\n",
        "        print(\"\\nSummary:\")\n",
        "        print(f\"Total Departments: {len(files_by_dept)}\")\n",
        "        print(f\"Total Files: {total_files}\")\n",
        "\n",
        "        # Save URLs to a CSV file for easy reference\n",
        "        url_data = []\n",
        "        for dept in files_by_dept:\n",
        "            for doc_type in files_by_dept[dept]:\n",
        "                for file_info in files_by_dept[dept][doc_type]:\n",
        "                    url_data.append({\n",
        "                        'Department': dept,\n",
        "                        'Document Type': doc_type,\n",
        "                        'File Name': file_info['file_name'],\n",
        "                        'S3 Path': file_info['s3_path'],\n",
        "                        'Download URL': file_info['download_url'],\n",
        "                        'Last Modified': file_info['last_modified']\n",
        "                    })\n",
        "\n",
        "        df = pd.DataFrame(url_data)\n",
        "        csv_filename = 's3_file_urls.csv'\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        print(f\"\\nURLs have been saved to {csv_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing S3 files: {str(e)}\")\n",
        "\n",
        "# Run the function\n",
        "list_s3_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s81x_RbcRMZ",
        "outputId": "37d07d57-4660-4870-ec9f-a7883cd08148"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing all files in S3 bucket with download URLs...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Department: ENGINEERING\n",
            "==================================================\n",
            "\n",
            "Document Type: payslip\n",
            "------------------------------\n",
            "File: DOC_2_PAYSLIP_20250401112013.txt\n",
            "Size: 162 bytes\n",
            "Last Modified: 2025-04-01 11:22:22+00:00\n",
            "S3 Path: engineering/payslip/DOC_2_PAYSLIP_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/engineering/payslip/DOC_2_PAYSLIP_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: resignation\n",
            "------------------------------\n",
            "File: DOC_4_RESIGNATION_20250401112013.txt\n",
            "Size: 170 bytes\n",
            "Last Modified: 2025-04-01 11:22:24+00:00\n",
            "S3 Path: engineering/resignation/DOC_4_RESIGNATION_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/engineering/resignation/DOC_4_RESIGNATION_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: visa\n",
            "------------------------------\n",
            "File: DOC_2_VISA_20250401112013.txt\n",
            "Size: 153 bytes\n",
            "Last Modified: 2025-04-01 11:22:22+00:00\n",
            "S3 Path: engineering/visa/DOC_2_VISA_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/engineering/visa/DOC_2_VISA_20250401112013.txt\n",
            "\n",
            "\n",
            "Department: FINANCE\n",
            "==================================================\n",
            "\n",
            "Document Type: contract\n",
            "------------------------------\n",
            "File: DOC_3_CONTRACT_20250401112013.txt\n",
            "Size: 158 bytes\n",
            "Last Modified: 2025-04-01 11:22:23+00:00\n",
            "S3 Path: finance/contract/DOC_3_CONTRACT_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/contract/DOC_3_CONTRACT_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: image\n",
            "------------------------------\n",
            "File: DOC_2_IMAGE_20250403083552.jpeg\n",
            "Size: 102 bytes\n",
            "Last Modified: 2025-04-03 08:37:12+00:00\n",
            "S3 Path: finance/image/DOC_2_IMAGE_20250403083552.jpeg\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/image/DOC_2_IMAGE_20250403083552.jpeg\n",
            "\n",
            "\n",
            "Document Type: passport\n",
            "------------------------------\n",
            "File: DOC_3_PASSPORT_20250401112013.txt\n",
            "Size: 158 bytes\n",
            "Last Modified: 2025-04-01 11:22:23+00:00\n",
            "S3 Path: finance/passport/DOC_3_PASSPORT_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/passport/DOC_3_PASSPORT_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: payslip\n",
            "------------------------------\n",
            "File: DOC_2_PAYSLIP_20250401114446.pdf\n",
            "Size: 100 bytes\n",
            "Last Modified: 2025-04-01 11:45:07+00:00\n",
            "S3 Path: finance/payslip/DOC_2_PAYSLIP_20250401114446.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/payslip/DOC_2_PAYSLIP_20250401114446.pdf\n",
            "\n",
            "File: DOC_2_PAYSLIP_20250403072945.pdf\n",
            "Size: 100 bytes\n",
            "Last Modified: 2025-04-03 07:30:04+00:00\n",
            "S3 Path: finance/payslip/DOC_2_PAYSLIP_20250403072945.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/payslip/DOC_2_PAYSLIP_20250403072945.pdf\n",
            "\n",
            "File: DOC_2_PAYSLIP_20250403083552.pdf\n",
            "Size: 100 bytes\n",
            "Last Modified: 2025-04-03 08:37:12+00:00\n",
            "S3 Path: finance/payslip/DOC_2_PAYSLIP_20250403083552.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/payslip/DOC_2_PAYSLIP_20250403083552.pdf\n",
            "\n",
            "\n",
            "Document Type: word\n",
            "------------------------------\n",
            "File: DOC_2_WORD_20250401114446.docx\n",
            "Size: 96 bytes\n",
            "Last Modified: 2025-04-01 11:45:06+00:00\n",
            "S3 Path: finance/word/DOC_2_WORD_20250401114446.docx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/word/DOC_2_WORD_20250401114446.docx\n",
            "\n",
            "File: DOC_2_WORD_20250403072945.docx\n",
            "Size: 96 bytes\n",
            "Last Modified: 2025-04-03 07:30:04+00:00\n",
            "S3 Path: finance/word/DOC_2_WORD_20250403072945.docx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/word/DOC_2_WORD_20250403072945.docx\n",
            "\n",
            "File: DOC_2_WORD_20250403083552.docx\n",
            "Size: 96 bytes\n",
            "Last Modified: 2025-04-03 08:37:12+00:00\n",
            "S3 Path: finance/word/DOC_2_WORD_20250403083552.docx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/finance/word/DOC_2_WORD_20250403083552.docx\n",
            "\n",
            "\n",
            "Department: HR\n",
            "==================================================\n",
            "\n",
            "Document Type: contract\n",
            "------------------------------\n",
            "File: DOC_1_CONTRACT_20250401112013.txt\n",
            "Size: 150 bytes\n",
            "Last Modified: 2025-04-01 11:22:21+00:00\n",
            "S3 Path: hr/contract/DOC_1_CONTRACT_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/contract/DOC_1_CONTRACT_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: excel\n",
            "------------------------------\n",
            "File: DOC_1_EXCEL_20250401114446.xlsx\n",
            "Size: 112 bytes\n",
            "Last Modified: 2025-04-01 11:45:05+00:00\n",
            "S3 Path: hr/excel/DOC_1_EXCEL_20250401114446.xlsx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/excel/DOC_1_EXCEL_20250401114446.xlsx\n",
            "\n",
            "File: DOC_1_EXCEL_20250403072945.xlsx\n",
            "Size: 112 bytes\n",
            "Last Modified: 2025-04-03 07:30:03+00:00\n",
            "S3 Path: hr/excel/DOC_1_EXCEL_20250403072945.xlsx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/excel/DOC_1_EXCEL_20250403072945.xlsx\n",
            "\n",
            "File: DOC_1_EXCEL_20250403083552.xlsx\n",
            "Size: 112 bytes\n",
            "Last Modified: 2025-04-03 08:37:10+00:00\n",
            "S3 Path: hr/excel/DOC_1_EXCEL_20250403083552.xlsx\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/excel/DOC_1_EXCEL_20250403083552.xlsx\n",
            "\n",
            "\n",
            "Document Type: image\n",
            "------------------------------\n",
            "File: DOC_1_IMAGE_20250403083552.png\n",
            "Size: 98 bytes\n",
            "Last Modified: 2025-04-03 08:37:11+00:00\n",
            "S3 Path: hr/image/DOC_1_IMAGE_20250403083552.png\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/image/DOC_1_IMAGE_20250403083552.png\n",
            "\n",
            "\n",
            "Document Type: passport\n",
            "------------------------------\n",
            "File: DOC_1_PASSPORT_20250401112013.txt\n",
            "Size: 150 bytes\n",
            "Last Modified: 2025-04-01 11:22:21+00:00\n",
            "S3 Path: hr/passport/DOC_1_PASSPORT_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/passport/DOC_1_PASSPORT_20250401112013.txt\n",
            "\n",
            "\n",
            "Document Type: pdf\n",
            "------------------------------\n",
            "File: DOC_1_PDF_20250401114446.pdf\n",
            "Size: 115 bytes\n",
            "Last Modified: 2025-04-01 11:45:06+00:00\n",
            "S3 Path: hr/pdf/DOC_1_PDF_20250401114446.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/pdf/DOC_1_PDF_20250401114446.pdf\n",
            "\n",
            "File: DOC_1_PDF_20250403072945.pdf\n",
            "Size: 115 bytes\n",
            "Last Modified: 2025-04-03 07:30:04+00:00\n",
            "S3 Path: hr/pdf/DOC_1_PDF_20250403072945.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/pdf/DOC_1_PDF_20250403072945.pdf\n",
            "\n",
            "File: DOC_1_PDF_20250403083552.pdf\n",
            "Size: 115 bytes\n",
            "Last Modified: 2025-04-03 08:37:11+00:00\n",
            "S3 Path: hr/pdf/DOC_1_PDF_20250403083552.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/hr/pdf/DOC_1_PDF_20250403083552.pdf\n",
            "\n",
            "\n",
            "Department: MARKETING\n",
            "==================================================\n",
            "\n",
            "Document Type: contract\n",
            "------------------------------\n",
            "File: DOC_5_CONTRACT_20250401112013.txt\n",
            "Size: 162 bytes\n",
            "Last Modified: 2025-04-01 11:22:24+00:00\n",
            "S3 Path: marketing/contract/DOC_5_CONTRACT_20250401112013.txt\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/marketing/contract/DOC_5_CONTRACT_20250401112013.txt\n",
            "\n",
            "\n",
            "Department: OPERATIONS\n",
            "==================================================\n",
            "\n",
            "Document Type: image\n",
            "------------------------------\n",
            "File: DOC_3_IMAGE_20250403083552.jpg\n",
            "Size: 101 bytes\n",
            "Last Modified: 2025-04-03 08:37:14+00:00\n",
            "S3 Path: operations/image/DOC_3_IMAGE_20250403083552.jpg\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/image/DOC_3_IMAGE_20250403083552.jpg\n",
            "\n",
            "\n",
            "Document Type: position\n",
            "------------------------------\n",
            "File: DOC_3_POSITION_20250401114446.pdf\n",
            "Size: 118 bytes\n",
            "Last Modified: 2025-04-01 11:45:07+00:00\n",
            "S3 Path: operations/position/DOC_3_POSITION_20250401114446.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/position/DOC_3_POSITION_20250401114446.pdf\n",
            "\n",
            "File: DOC_3_POSITION_20250403072945.pdf\n",
            "Size: 118 bytes\n",
            "Last Modified: 2025-04-03 07:30:05+00:00\n",
            "S3 Path: operations/position/DOC_3_POSITION_20250403072945.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/position/DOC_3_POSITION_20250403072945.pdf\n",
            "\n",
            "File: DOC_3_POSITION_20250403083552.pdf\n",
            "Size: 118 bytes\n",
            "Last Modified: 2025-04-03 08:37:13+00:00\n",
            "S3 Path: operations/position/DOC_3_POSITION_20250403083552.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/position/DOC_3_POSITION_20250403083552.pdf\n",
            "\n",
            "\n",
            "Document Type: salary\n",
            "------------------------------\n",
            "File: DOC_3_SALARY_20250401114446.pdf\n",
            "Size: 102 bytes\n",
            "Last Modified: 2025-04-01 11:45:08+00:00\n",
            "S3 Path: operations/salary/DOC_3_SALARY_20250401114446.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/salary/DOC_3_SALARY_20250401114446.pdf\n",
            "\n",
            "File: DOC_3_SALARY_20250403072945.pdf\n",
            "Size: 102 bytes\n",
            "Last Modified: 2025-04-03 07:30:05+00:00\n",
            "S3 Path: operations/salary/DOC_3_SALARY_20250403072945.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/salary/DOC_3_SALARY_20250403072945.pdf\n",
            "\n",
            "File: DOC_3_SALARY_20250403083552.pdf\n",
            "Size: 102 bytes\n",
            "Last Modified: 2025-04-03 08:37:13+00:00\n",
            "S3 Path: operations/salary/DOC_3_SALARY_20250403083552.pdf\n",
            "Download URL: https://testempdoc.s3.ap-south-1.amazonaws.com/operations/salary/DOC_3_SALARY_20250403083552.pdf\n",
            "\n",
            "\n",
            "Summary:\n",
            "Total Departments: 5\n",
            "Total Files: 29\n",
            "\n",
            "URLs have been saved to s3_file_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9J97yw3cXpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}